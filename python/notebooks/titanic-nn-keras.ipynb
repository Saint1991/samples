{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f84a32a-c11f-40f5-bd80-e461b930dce4",
   "metadata": {},
   "source": [
    "# ロジスティック回帰で予測するサンプル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab94e637-73ef-4468-aa65-df5ceae2e4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e204868-29fe-44fc-9e55-87d7c911b1f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>892</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Kelly, Mr. James</td>\n",
       "      <td>male</td>\n",
       "      <td>34.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>330911</td>\n",
       "      <td>7.8292</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>893</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Wilkes, Mrs. James (Ellen Needs)</td>\n",
       "      <td>female</td>\n",
       "      <td>47.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>363272</td>\n",
       "      <td>7.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>894</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Myles, Mr. Thomas Francis</td>\n",
       "      <td>male</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>240276</td>\n",
       "      <td>9.6875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>895</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Wirz, Mr. Albert</td>\n",
       "      <td>male</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>315154</td>\n",
       "      <td>8.6625</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>896</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Hirvonen, Mrs. Alexander (Helga E Lindqvist)</td>\n",
       "      <td>female</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3101298</td>\n",
       "      <td>12.2875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>1305</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Spector, Mr. Woolf</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>A.5. 3236</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>1306</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Oliva y Ocana, Dona. Fermina</td>\n",
       "      <td>female</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17758</td>\n",
       "      <td>108.9000</td>\n",
       "      <td>C105</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>1307</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Saether, Mr. Simon Sivertsen</td>\n",
       "      <td>male</td>\n",
       "      <td>38.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>SOTON/O.Q. 3101262</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>1308</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Ware, Mr. Frederick</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>359309</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>1309</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Peter, Master. Michael J</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2668</td>\n",
       "      <td>22.3583</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>418 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Survived  Pclass  \\\n",
       "0            892         0       3   \n",
       "1            893         1       3   \n",
       "2            894         0       2   \n",
       "3            895         0       3   \n",
       "4            896         1       3   \n",
       "..           ...       ...     ...   \n",
       "413         1305         0       3   \n",
       "414         1306         1       1   \n",
       "415         1307         0       3   \n",
       "416         1308         0       3   \n",
       "417         1309         0       3   \n",
       "\n",
       "                                             Name     Sex   Age  SibSp  Parch  \\\n",
       "0                                Kelly, Mr. James    male  34.5      0      0   \n",
       "1                Wilkes, Mrs. James (Ellen Needs)  female  47.0      1      0   \n",
       "2                       Myles, Mr. Thomas Francis    male  62.0      0      0   \n",
       "3                                Wirz, Mr. Albert    male  27.0      0      0   \n",
       "4    Hirvonen, Mrs. Alexander (Helga E Lindqvist)  female  22.0      1      1   \n",
       "..                                            ...     ...   ...    ...    ...   \n",
       "413                            Spector, Mr. Woolf    male   NaN      0      0   \n",
       "414                  Oliva y Ocana, Dona. Fermina  female  39.0      0      0   \n",
       "415                  Saether, Mr. Simon Sivertsen    male  38.5      0      0   \n",
       "416                           Ware, Mr. Frederick    male   NaN      0      0   \n",
       "417                      Peter, Master. Michael J    male   NaN      1      1   \n",
       "\n",
       "                 Ticket      Fare Cabin Embarked  \n",
       "0                330911    7.8292   NaN        Q  \n",
       "1                363272    7.0000   NaN        S  \n",
       "2                240276    9.6875   NaN        Q  \n",
       "3                315154    8.6625   NaN        S  \n",
       "4               3101298   12.2875   NaN        S  \n",
       "..                  ...       ...   ...      ...  \n",
       "413           A.5. 3236    8.0500   NaN        S  \n",
       "414            PC 17758  108.9000  C105        C  \n",
       "415  SOTON/O.Q. 3101262    7.2500   NaN        S  \n",
       "416              359309    8.0500   NaN        S  \n",
       "417                2668   22.3583   NaN        C  \n",
       "\n",
       "[418 rows x 12 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./datasets/titanic/train.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b9713a0-d3a2-4fe4-8b5c-3db019c5eaca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>418.000000</td>\n",
       "      <td>418.000000</td>\n",
       "      <td>418.000000</td>\n",
       "      <td>332.000000</td>\n",
       "      <td>418.000000</td>\n",
       "      <td>418.000000</td>\n",
       "      <td>417.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1100.500000</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>2.265550</td>\n",
       "      <td>30.272590</td>\n",
       "      <td>0.447368</td>\n",
       "      <td>0.392344</td>\n",
       "      <td>35.627188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>120.810458</td>\n",
       "      <td>0.481622</td>\n",
       "      <td>0.841838</td>\n",
       "      <td>14.181209</td>\n",
       "      <td>0.896760</td>\n",
       "      <td>0.981429</td>\n",
       "      <td>55.907576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>892.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.170000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>996.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.895800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1100.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.454200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1204.750000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1309.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>512.329200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       PassengerId    Survived      Pclass         Age       SibSp  \\\n",
       "count   418.000000  418.000000  418.000000  332.000000  418.000000   \n",
       "mean   1100.500000    0.363636    2.265550   30.272590    0.447368   \n",
       "std     120.810458    0.481622    0.841838   14.181209    0.896760   \n",
       "min     892.000000    0.000000    1.000000    0.170000    0.000000   \n",
       "25%     996.250000    0.000000    1.000000   21.000000    0.000000   \n",
       "50%    1100.500000    0.000000    3.000000   27.000000    0.000000   \n",
       "75%    1204.750000    1.000000    3.000000   39.000000    1.000000   \n",
       "max    1309.000000    1.000000    3.000000   76.000000    8.000000   \n",
       "\n",
       "            Parch        Fare  \n",
       "count  418.000000  417.000000  \n",
       "mean     0.392344   35.627188  \n",
       "std      0.981429   55.907576  \n",
       "min      0.000000    0.000000  \n",
       "25%      0.000000    7.895800  \n",
       "50%      0.000000   14.454200  \n",
       "75%      0.000000   31.500000  \n",
       "max      9.000000  512.329200  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81ee4c43-8096-4c8b-b355-e26c36c7d17c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>892</td>\n",
       "      <td>3</td>\n",
       "      <td>Kelly, Mr. James</td>\n",
       "      <td>male</td>\n",
       "      <td>34.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>330911</td>\n",
       "      <td>7.8292</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>893</td>\n",
       "      <td>3</td>\n",
       "      <td>Wilkes, Mrs. James (Ellen Needs)</td>\n",
       "      <td>female</td>\n",
       "      <td>47.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>363272</td>\n",
       "      <td>7.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>894</td>\n",
       "      <td>2</td>\n",
       "      <td>Myles, Mr. Thomas Francis</td>\n",
       "      <td>male</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>240276</td>\n",
       "      <td>9.6875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>895</td>\n",
       "      <td>3</td>\n",
       "      <td>Wirz, Mr. Albert</td>\n",
       "      <td>male</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>315154</td>\n",
       "      <td>8.6625</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>896</td>\n",
       "      <td>3</td>\n",
       "      <td>Hirvonen, Mrs. Alexander (Helga E Lindqvist)</td>\n",
       "      <td>female</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3101298</td>\n",
       "      <td>12.2875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>1305</td>\n",
       "      <td>3</td>\n",
       "      <td>Spector, Mr. Woolf</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>A.5. 3236</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>1306</td>\n",
       "      <td>1</td>\n",
       "      <td>Oliva y Ocana, Dona. Fermina</td>\n",
       "      <td>female</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17758</td>\n",
       "      <td>108.9000</td>\n",
       "      <td>C105</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>1307</td>\n",
       "      <td>3</td>\n",
       "      <td>Saether, Mr. Simon Sivertsen</td>\n",
       "      <td>male</td>\n",
       "      <td>38.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>SOTON/O.Q. 3101262</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>1308</td>\n",
       "      <td>3</td>\n",
       "      <td>Ware, Mr. Frederick</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>359309</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>1309</td>\n",
       "      <td>3</td>\n",
       "      <td>Peter, Master. Michael J</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2668</td>\n",
       "      <td>22.3583</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>418 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Pclass                                          Name  \\\n",
       "0            892       3                              Kelly, Mr. James   \n",
       "1            893       3              Wilkes, Mrs. James (Ellen Needs)   \n",
       "2            894       2                     Myles, Mr. Thomas Francis   \n",
       "3            895       3                              Wirz, Mr. Albert   \n",
       "4            896       3  Hirvonen, Mrs. Alexander (Helga E Lindqvist)   \n",
       "..           ...     ...                                           ...   \n",
       "413         1305       3                            Spector, Mr. Woolf   \n",
       "414         1306       1                  Oliva y Ocana, Dona. Fermina   \n",
       "415         1307       3                  Saether, Mr. Simon Sivertsen   \n",
       "416         1308       3                           Ware, Mr. Frederick   \n",
       "417         1309       3                      Peter, Master. Michael J   \n",
       "\n",
       "        Sex   Age  SibSp  Parch              Ticket      Fare Cabin Embarked  \n",
       "0      male  34.5      0      0              330911    7.8292   NaN        Q  \n",
       "1    female  47.0      1      0              363272    7.0000   NaN        S  \n",
       "2      male  62.0      0      0              240276    9.6875   NaN        Q  \n",
       "3      male  27.0      0      0              315154    8.6625   NaN        S  \n",
       "4    female  22.0      1      1             3101298   12.2875   NaN        S  \n",
       "..      ...   ...    ...    ...                 ...       ...   ...      ...  \n",
       "413    male   NaN      0      0           A.5. 3236    8.0500   NaN        S  \n",
       "414  female  39.0      0      0            PC 17758  108.9000  C105        C  \n",
       "415    male  38.5      0      0  SOTON/O.Q. 3101262    7.2500   NaN        S  \n",
       "416    male   NaN      0      0              359309    8.0500   NaN        S  \n",
       "417    male   NaN      1      1                2668   22.3583   NaN        C  \n",
       "\n",
       "[418 rows x 11 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv('./datasets/titanic/test.csv')\n",
    "test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3fa7b77-f23f-4bbd-b769-1cb7a1d557f1",
   "metadata": {},
   "source": [
    "## 名前の前処理family name, last nameに分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "577ddb65-1629-47b0-8ff7-be6b90cb90fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "title_pattern = re.compile('[a-zA-Z]+\\.\\s')\n",
    "LAST='LastName'\n",
    "FAMILY='FamilyName'\n",
    "\n",
    "def _preprocess_name(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    name_df = df['Name'].str.split(',', expand=True)\n",
    "    name_df.columns = [LAST, FAMILY]\n",
    "    name_df[FAMILY] = name_df[FAMILY].str.replace(title_pattern, '', regex=True).str.strip()\n",
    "    df = df.drop(columns='Name')\n",
    "    name_df = name_df.drop(columns='LastName')\n",
    "    return pd.concat([df, name_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba0411d-4b07-4e06-b3da-cad42f0f7c4b",
   "metadata": {},
   "source": [
    "## pclassの前処理 欠損埋めしてスケーリング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46860f5f-8800-4b59-9edb-d957f3f4c4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "PCLASS= 'Pclass'\n",
    "\n",
    "def _preprocess_pclass(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df[PCLASS] = df[PCLASS].fillna(0)\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(df[[PCLASS]])\n",
    "    \n",
    "    df[PCLASS] = scaler.transform(df[[PCLASS]])\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd9d5c6-1023-41ca-a7e9-2c4cbec60955",
   "metadata": {},
   "source": [
    "## Age, Fareの前処理　欠損埋めしてスケーリング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7455f60e-771d-4af4-bb09-1975d128631f",
   "metadata": {},
   "outputs": [],
   "source": [
    "AGE = 'Age'\n",
    "FARE = 'Fare'\n",
    "SIBSP = 'SibSp'\n",
    "PARCH = 'Parch'\n",
    "\n",
    "\n",
    "def _preprocess_age_and_fare_sibsp_parch(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df['Age_isnull'] = df[AGE].isnull().astype(int)\n",
    "    df['Fare_isnull'] = df[FARE].isnull().astype(int)\n",
    "    \n",
    "    mean_age = df[AGE].mean()\n",
    "    mean_fare = df[FARE].mean()\n",
    "    \n",
    "    df[AGE] = df[AGE].fillna(mean_age)\n",
    "    df[FARE] = df[FARE].fillna(mean_fare)\n",
    "    df[SIBSP] = df[SIBSP].fillna(0)\n",
    "    df[PARCH] = df[PARCH].fillna(0)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(df[[AGE, FARE, SIBSP, PARCH]])\n",
    "    \n",
    "    df[[AGE, FARE, SIBSP, PARCH]] = scaler.transform(df[[AGE, FARE, SIBSP, PARCH]])\n",
    "    return df\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83925d21-8a45-4a20-b4f1-accd25b32c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _preprocess_sex_cabin_embarked(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    dummy_df = pd.get_dummies(df, drop_first=True, dummy_na=True, columns=['Sex', 'Cabin', 'Embarked'])\n",
    "    return dummy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f536b117-704e-4f94-9bc4-ab8e4b9ff03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.drop(columns=['PassengerId', 'Ticket', 'Name'])\n",
    "\n",
    "    # df = _preprocess_name(df)\n",
    "    df = _preprocess_pclass(df)\n",
    "    df = _preprocess_age_and_fare_sibsp_parch(df)\n",
    "    df = _preprocess_sex_cabin_embarked(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11915a86-a6b9-4c38-b85c-c0a67e39eebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_df = preprocess(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5be9d978-975d-4878-adc3-11b4a10b9d8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Age_isnull</th>\n",
       "      <th>Fare_isnull</th>\n",
       "      <th>Sex_male</th>\n",
       "      <th>Sex_nan</th>\n",
       "      <th>...</th>\n",
       "      <th>Cabin_F E57</th>\n",
       "      <th>Cabin_F G63</th>\n",
       "      <th>Cabin_F2</th>\n",
       "      <th>Cabin_F33</th>\n",
       "      <th>Cabin_F4</th>\n",
       "      <th>Cabin_G6</th>\n",
       "      <th>Cabin_nan</th>\n",
       "      <th>Embarked_Q</th>\n",
       "      <th>Embarked_S</th>\n",
       "      <th>Embarked_nan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>418.000000</td>\n",
       "      <td>418.000000</td>\n",
       "      <td>4.180000e+02</td>\n",
       "      <td>4.180000e+02</td>\n",
       "      <td>4.180000e+02</td>\n",
       "      <td>4.180000e+02</td>\n",
       "      <td>418.000000</td>\n",
       "      <td>418.000000</td>\n",
       "      <td>418.000000</td>\n",
       "      <td>418.0</td>\n",
       "      <td>...</td>\n",
       "      <td>418.000000</td>\n",
       "      <td>418.000000</td>\n",
       "      <td>418.000000</td>\n",
       "      <td>418.000000</td>\n",
       "      <td>418.000000</td>\n",
       "      <td>418.000000</td>\n",
       "      <td>418.000000</td>\n",
       "      <td>418.000000</td>\n",
       "      <td>418.000000</td>\n",
       "      <td>418.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.632775</td>\n",
       "      <td>3.187243e-17</td>\n",
       "      <td>-1.593622e-17</td>\n",
       "      <td>-2.974760e-17</td>\n",
       "      <td>-1.806104e-17</td>\n",
       "      <td>0.205742</td>\n",
       "      <td>0.002392</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002392</td>\n",
       "      <td>0.002392</td>\n",
       "      <td>0.002392</td>\n",
       "      <td>0.002392</td>\n",
       "      <td>0.004785</td>\n",
       "      <td>0.002392</td>\n",
       "      <td>0.782297</td>\n",
       "      <td>0.110048</td>\n",
       "      <td>0.645933</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.481622</td>\n",
       "      <td>0.420919</td>\n",
       "      <td>1.001198e+00</td>\n",
       "      <td>1.001198e+00</td>\n",
       "      <td>1.001198e+00</td>\n",
       "      <td>1.001198e+00</td>\n",
       "      <td>0.404727</td>\n",
       "      <td>0.048912</td>\n",
       "      <td>0.481622</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.048912</td>\n",
       "      <td>0.048912</td>\n",
       "      <td>0.048912</td>\n",
       "      <td>0.048912</td>\n",
       "      <td>0.069088</td>\n",
       "      <td>0.048912</td>\n",
       "      <td>0.413179</td>\n",
       "      <td>0.313324</td>\n",
       "      <td>0.478803</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.385419e+00</td>\n",
       "      <td>-4.994700e-01</td>\n",
       "      <td>-4.002477e-01</td>\n",
       "      <td>-6.387815e-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.763018e-01</td>\n",
       "      <td>-4.994700e-01</td>\n",
       "      <td>-4.002477e-01</td>\n",
       "      <td>-4.972129e-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-4.994700e-01</td>\n",
       "      <td>-4.002477e-01</td>\n",
       "      <td>-3.796234e-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.340463e-01</td>\n",
       "      <td>6.169924e-01</td>\n",
       "      <td>-4.002477e-01</td>\n",
       "      <td>-7.399887e-02</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.623577e+00</td>\n",
       "      <td>8.432229e+00</td>\n",
       "      <td>8.781044e+00</td>\n",
       "      <td>8.547081e+00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 89 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Survived      Pclass           Age         SibSp         Parch  \\\n",
       "count  418.000000  418.000000  4.180000e+02  4.180000e+02  4.180000e+02   \n",
       "mean     0.363636    0.632775  3.187243e-17 -1.593622e-17 -2.974760e-17   \n",
       "std      0.481622    0.420919  1.001198e+00  1.001198e+00  1.001198e+00   \n",
       "min      0.000000    0.000000 -2.385419e+00 -4.994700e-01 -4.002477e-01   \n",
       "25%      0.000000    0.000000 -5.763018e-01 -4.994700e-01 -4.002477e-01   \n",
       "50%      0.000000    1.000000  0.000000e+00 -4.994700e-01 -4.002477e-01   \n",
       "75%      1.000000    1.000000  4.340463e-01  6.169924e-01 -4.002477e-01   \n",
       "max      1.000000    1.000000  3.623577e+00  8.432229e+00  8.781044e+00   \n",
       "\n",
       "               Fare  Age_isnull  Fare_isnull    Sex_male  Sex_nan  ...  \\\n",
       "count  4.180000e+02  418.000000   418.000000  418.000000    418.0  ...   \n",
       "mean  -1.806104e-17    0.205742     0.002392    0.636364      0.0  ...   \n",
       "std    1.001198e+00    0.404727     0.048912    0.481622      0.0  ...   \n",
       "min   -6.387815e-01    0.000000     0.000000    0.000000      0.0  ...   \n",
       "25%   -4.972129e-01    0.000000     0.000000    0.000000      0.0  ...   \n",
       "50%   -3.796234e-01    0.000000     0.000000    1.000000      0.0  ...   \n",
       "75%   -7.399887e-02    0.000000     0.000000    1.000000      0.0  ...   \n",
       "max    8.547081e+00    1.000000     1.000000    1.000000      0.0  ...   \n",
       "\n",
       "       Cabin_F E57  Cabin_F G63    Cabin_F2   Cabin_F33    Cabin_F4  \\\n",
       "count   418.000000   418.000000  418.000000  418.000000  418.000000   \n",
       "mean      0.002392     0.002392    0.002392    0.002392    0.004785   \n",
       "std       0.048912     0.048912    0.048912    0.048912    0.069088   \n",
       "min       0.000000     0.000000    0.000000    0.000000    0.000000   \n",
       "25%       0.000000     0.000000    0.000000    0.000000    0.000000   \n",
       "50%       0.000000     0.000000    0.000000    0.000000    0.000000   \n",
       "75%       0.000000     0.000000    0.000000    0.000000    0.000000   \n",
       "max       1.000000     1.000000    1.000000    1.000000    1.000000   \n",
       "\n",
       "         Cabin_G6   Cabin_nan  Embarked_Q  Embarked_S  Embarked_nan  \n",
       "count  418.000000  418.000000  418.000000  418.000000         418.0  \n",
       "mean     0.002392    0.782297    0.110048    0.645933           0.0  \n",
       "std      0.048912    0.413179    0.313324    0.478803           0.0  \n",
       "min      0.000000    0.000000    0.000000    0.000000           0.0  \n",
       "25%      0.000000    1.000000    0.000000    0.000000           0.0  \n",
       "50%      0.000000    1.000000    0.000000    1.000000           0.0  \n",
       "75%      0.000000    1.000000    0.000000    1.000000           0.0  \n",
       "max      1.000000    1.000000    1.000000    1.000000           0.0  \n",
       "\n",
       "[8 rows x 89 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prep_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d90e2cd-0053-4b0c-90b9-3f5bacd65e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86729a5-4b1d-4d17-9175-c2f127ad7cb8",
   "metadata": {},
   "source": [
    "## 25%でhold-outしてバリデーションデータを作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e23e8a0d-f3ec-4ec3-bded-13c9beeb1f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ① 学習用データとバリデーションデータを用意\n",
    "df_x = prep_df.drop(columns=['Survived'])\n",
    "df_y = prep_df['Survived']\n",
    "\n",
    "train_x, valid_x, train_y, valid_y = train_test_split(df_x, df_y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f62e4e-4306-410d-bbf8-22f58dabc710",
   "metadata": {},
   "source": [
    "## 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "143ee8fa-9feb-47f8-90c1-cb903e5e76ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "10/10 [==============================] - 1s 19ms/step - loss: 0.8082 - binary_crossentropy: 0.8082 - val_loss: 0.6770 - val_binary_crossentropy: 0.6770\n",
      "Epoch 2/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.7015 - binary_crossentropy: 0.7015 - val_loss: 0.6500 - val_binary_crossentropy: 0.6500\n",
      "Epoch 3/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.5961 - binary_crossentropy: 0.5961 - val_loss: 0.6290 - val_binary_crossentropy: 0.6290\n",
      "Epoch 4/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.5080 - binary_crossentropy: 0.5080 - val_loss: 0.6025 - val_binary_crossentropy: 0.6025\n",
      "Epoch 5/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.4361 - binary_crossentropy: 0.4361 - val_loss: 0.5800 - val_binary_crossentropy: 0.5800\n",
      "Epoch 6/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.4170 - binary_crossentropy: 0.4170 - val_loss: 0.5578 - val_binary_crossentropy: 0.5578\n",
      "Epoch 7/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.3225 - binary_crossentropy: 0.3225 - val_loss: 0.5357 - val_binary_crossentropy: 0.5357\n",
      "Epoch 8/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.3171 - binary_crossentropy: 0.3171 - val_loss: 0.5133 - val_binary_crossentropy: 0.5133\n",
      "Epoch 9/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.2693 - binary_crossentropy: 0.2693 - val_loss: 0.4934 - val_binary_crossentropy: 0.4934\n",
      "Epoch 10/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.2857 - binary_crossentropy: 0.2857 - val_loss: 0.4741 - val_binary_crossentropy: 0.4741\n",
      "Epoch 11/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.2405 - binary_crossentropy: 0.2405 - val_loss: 0.4530 - val_binary_crossentropy: 0.4530\n",
      "Epoch 12/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.2001 - binary_crossentropy: 0.2001 - val_loss: 0.4325 - val_binary_crossentropy: 0.4325\n",
      "Epoch 13/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.2328 - binary_crossentropy: 0.2328 - val_loss: 0.4122 - val_binary_crossentropy: 0.4122\n",
      "Epoch 14/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.2063 - binary_crossentropy: 0.2063 - val_loss: 0.3917 - val_binary_crossentropy: 0.3917\n",
      "Epoch 15/200\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1881 - binary_crossentropy: 0.1881 - val_loss: 0.3681 - val_binary_crossentropy: 0.3681\n",
      "Epoch 16/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1941 - binary_crossentropy: 0.1941 - val_loss: 0.3493 - val_binary_crossentropy: 0.3493\n",
      "Epoch 17/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1661 - binary_crossentropy: 0.1661 - val_loss: 0.3330 - val_binary_crossentropy: 0.3330\n",
      "Epoch 18/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1335 - binary_crossentropy: 0.1335 - val_loss: 0.3131 - val_binary_crossentropy: 0.3131\n",
      "Epoch 19/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1433 - binary_crossentropy: 0.1433 - val_loss: 0.2931 - val_binary_crossentropy: 0.2931\n",
      "Epoch 20/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1336 - binary_crossentropy: 0.1336 - val_loss: 0.2784 - val_binary_crossentropy: 0.2784\n",
      "Epoch 21/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1209 - binary_crossentropy: 0.1209 - val_loss: 0.2660 - val_binary_crossentropy: 0.2660\n",
      "Epoch 22/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0955 - binary_crossentropy: 0.0955 - val_loss: 0.2477 - val_binary_crossentropy: 0.2477\n",
      "Epoch 23/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1205 - binary_crossentropy: 0.1205 - val_loss: 0.2299 - val_binary_crossentropy: 0.2299\n",
      "Epoch 24/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1234 - binary_crossentropy: 0.1234 - val_loss: 0.2128 - val_binary_crossentropy: 0.2128\n",
      "Epoch 25/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0924 - binary_crossentropy: 0.0924 - val_loss: 0.1962 - val_binary_crossentropy: 0.1962\n",
      "Epoch 26/200\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1071 - binary_crossentropy: 0.1071 - val_loss: 0.1805 - val_binary_crossentropy: 0.1805\n",
      "Epoch 27/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1124 - binary_crossentropy: 0.1124 - val_loss: 0.1679 - val_binary_crossentropy: 0.1679\n",
      "Epoch 28/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0840 - binary_crossentropy: 0.0840 - val_loss: 0.1554 - val_binary_crossentropy: 0.1554\n",
      "Epoch 29/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0979 - binary_crossentropy: 0.0979 - val_loss: 0.1421 - val_binary_crossentropy: 0.1421\n",
      "Epoch 30/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0695 - binary_crossentropy: 0.0695 - val_loss: 0.1278 - val_binary_crossentropy: 0.1278\n",
      "Epoch 31/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0782 - binary_crossentropy: 0.0782 - val_loss: 0.1177 - val_binary_crossentropy: 0.1177\n",
      "Epoch 32/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0892 - binary_crossentropy: 0.0892 - val_loss: 0.1056 - val_binary_crossentropy: 0.1056\n",
      "Epoch 33/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0776 - binary_crossentropy: 0.0776 - val_loss: 0.0977 - val_binary_crossentropy: 0.0977\n",
      "Epoch 34/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0644 - binary_crossentropy: 0.0644 - val_loss: 0.0917 - val_binary_crossentropy: 0.0917\n",
      "Epoch 35/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0549 - binary_crossentropy: 0.0549 - val_loss: 0.0856 - val_binary_crossentropy: 0.0856\n",
      "Epoch 36/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0726 - binary_crossentropy: 0.0726 - val_loss: 0.0768 - val_binary_crossentropy: 0.0768\n",
      "Epoch 37/200\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0615 - binary_crossentropy: 0.0615 - val_loss: 0.0708 - val_binary_crossentropy: 0.0708\n",
      "Epoch 38/200\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0551 - binary_crossentropy: 0.0551 - val_loss: 0.0648 - val_binary_crossentropy: 0.0648\n",
      "Epoch 39/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0756 - binary_crossentropy: 0.0756 - val_loss: 0.0546 - val_binary_crossentropy: 0.0546\n",
      "Epoch 40/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0498 - binary_crossentropy: 0.0498 - val_loss: 0.0504 - val_binary_crossentropy: 0.0504\n",
      "Epoch 41/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0564 - binary_crossentropy: 0.0564 - val_loss: 0.0470 - val_binary_crossentropy: 0.0470\n",
      "Epoch 42/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0489 - binary_crossentropy: 0.0489 - val_loss: 0.0440 - val_binary_crossentropy: 0.0440\n",
      "Epoch 43/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0545 - binary_crossentropy: 0.0545 - val_loss: 0.0414 - val_binary_crossentropy: 0.0414\n",
      "Epoch 44/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0494 - binary_crossentropy: 0.0494 - val_loss: 0.0385 - val_binary_crossentropy: 0.0385\n",
      "Epoch 45/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0504 - binary_crossentropy: 0.0504 - val_loss: 0.0361 - val_binary_crossentropy: 0.0361\n",
      "Epoch 46/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0558 - binary_crossentropy: 0.0558 - val_loss: 0.0342 - val_binary_crossentropy: 0.0342\n",
      "Epoch 47/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0447 - binary_crossentropy: 0.0447 - val_loss: 0.0319 - val_binary_crossentropy: 0.0319\n",
      "Epoch 48/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0616 - binary_crossentropy: 0.0616 - val_loss: 0.0322 - val_binary_crossentropy: 0.0322\n",
      "Epoch 49/200\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0410 - binary_crossentropy: 0.0410 - val_loss: 0.0295 - val_binary_crossentropy: 0.0295\n",
      "Epoch 50/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0470 - binary_crossentropy: 0.0470 - val_loss: 0.0263 - val_binary_crossentropy: 0.0263\n",
      "Epoch 51/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0384 - binary_crossentropy: 0.0384 - val_loss: 0.0249 - val_binary_crossentropy: 0.0249\n",
      "Epoch 52/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0427 - binary_crossentropy: 0.0427 - val_loss: 0.0248 - val_binary_crossentropy: 0.0248\n",
      "Epoch 53/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0462 - binary_crossentropy: 0.0462 - val_loss: 0.0231 - val_binary_crossentropy: 0.0231\n",
      "Epoch 54/200\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0528 - binary_crossentropy: 0.0528 - val_loss: 0.0220 - val_binary_crossentropy: 0.0220\n",
      "Epoch 55/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0527 - binary_crossentropy: 0.0527 - val_loss: 0.0171 - val_binary_crossentropy: 0.0171\n",
      "Epoch 56/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0358 - binary_crossentropy: 0.0358 - val_loss: 0.0195 - val_binary_crossentropy: 0.0195\n",
      "Epoch 57/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0402 - binary_crossentropy: 0.0402 - val_loss: 0.0214 - val_binary_crossentropy: 0.0214\n",
      "Epoch 58/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0481 - binary_crossentropy: 0.0481 - val_loss: 0.0161 - val_binary_crossentropy: 0.0161\n",
      "Epoch 59/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0359 - binary_crossentropy: 0.0359 - val_loss: 0.0164 - val_binary_crossentropy: 0.0164\n",
      "Epoch 60/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0403 - binary_crossentropy: 0.0403 - val_loss: 0.0173 - val_binary_crossentropy: 0.0173\n",
      "Epoch 61/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0335 - binary_crossentropy: 0.0335 - val_loss: 0.0170 - val_binary_crossentropy: 0.0170\n",
      "Epoch 62/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0334 - binary_crossentropy: 0.0334 - val_loss: 0.0182 - val_binary_crossentropy: 0.0182\n",
      "Epoch 63/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0291 - binary_crossentropy: 0.0291 - val_loss: 0.0166 - val_binary_crossentropy: 0.0166\n",
      "Epoch 64/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0432 - binary_crossentropy: 0.0432 - val_loss: 0.0136 - val_binary_crossentropy: 0.0136\n",
      "Epoch 65/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0372 - binary_crossentropy: 0.0372 - val_loss: 0.0127 - val_binary_crossentropy: 0.0127\n",
      "Epoch 66/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0381 - binary_crossentropy: 0.0381 - val_loss: 0.0115 - val_binary_crossentropy: 0.0115\n",
      "Epoch 67/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0306 - binary_crossentropy: 0.0306 - val_loss: 0.0113 - val_binary_crossentropy: 0.0113\n",
      "Epoch 68/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0247 - binary_crossentropy: 0.0247 - val_loss: 0.0136 - val_binary_crossentropy: 0.0136\n",
      "Epoch 69/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0311 - binary_crossentropy: 0.0311 - val_loss: 0.0126 - val_binary_crossentropy: 0.0126\n",
      "Epoch 70/200\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0274 - binary_crossentropy: 0.0274 - val_loss: 0.0131 - val_binary_crossentropy: 0.0131\n",
      "Epoch 71/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0360 - binary_crossentropy: 0.0360 - val_loss: 0.0115 - val_binary_crossentropy: 0.0115\n",
      "Epoch 72/200\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0336 - binary_crossentropy: 0.0336 - val_loss: 0.0100 - val_binary_crossentropy: 0.0100\n",
      "Epoch 73/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0269 - binary_crossentropy: 0.0269 - val_loss: 0.0089 - val_binary_crossentropy: 0.0089\n",
      "Epoch 74/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0365 - binary_crossentropy: 0.0365 - val_loss: 0.0074 - val_binary_crossentropy: 0.0074\n",
      "Epoch 75/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0333 - binary_crossentropy: 0.0333 - val_loss: 0.0073 - val_binary_crossentropy: 0.0073\n",
      "Epoch 76/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0163 - binary_crossentropy: 0.0163 - val_loss: 0.0072 - val_binary_crossentropy: 0.0072\n",
      "Epoch 77/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0342 - binary_crossentropy: 0.0342 - val_loss: 0.0069 - val_binary_crossentropy: 0.0069\n",
      "Epoch 78/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0194 - binary_crossentropy: 0.0194 - val_loss: 0.0072 - val_binary_crossentropy: 0.0072\n",
      "Epoch 79/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0387 - binary_crossentropy: 0.0387 - val_loss: 0.0072 - val_binary_crossentropy: 0.0072\n",
      "Epoch 80/200\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0208 - binary_crossentropy: 0.0208 - val_loss: 0.0071 - val_binary_crossentropy: 0.0071\n",
      "Epoch 81/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0434 - binary_crossentropy: 0.0434 - val_loss: 0.0077 - val_binary_crossentropy: 0.0077\n",
      "Epoch 82/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0349 - binary_crossentropy: 0.0349 - val_loss: 0.0095 - val_binary_crossentropy: 0.0095\n",
      "Epoch 83/200\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0265 - binary_crossentropy: 0.0265 - val_loss: 0.0074 - val_binary_crossentropy: 0.0074\n",
      "Epoch 84/200\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0239 - binary_crossentropy: 0.0239 - val_loss: 0.0076 - val_binary_crossentropy: 0.0076\n",
      "Epoch 85/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0180 - binary_crossentropy: 0.0180 - val_loss: 0.0075 - val_binary_crossentropy: 0.0075\n",
      "Epoch 86/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0273 - binary_crossentropy: 0.0273 - val_loss: 0.0075 - val_binary_crossentropy: 0.0075\n",
      "Epoch 87/200\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0255 - binary_crossentropy: 0.0255 - val_loss: 0.0073 - val_binary_crossentropy: 0.0073\n",
      "Epoch 88/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0210 - binary_crossentropy: 0.0210 - val_loss: 0.0078 - val_binary_crossentropy: 0.0078\n",
      "Epoch 89/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0198 - binary_crossentropy: 0.0198 - val_loss: 0.0083 - val_binary_crossentropy: 0.0083\n",
      "Epoch 90/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0147 - binary_crossentropy: 0.0147 - val_loss: 0.0134 - val_binary_crossentropy: 0.0134\n",
      "Epoch 91/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0307 - binary_crossentropy: 0.0307 - val_loss: 0.0186 - val_binary_crossentropy: 0.0186\n",
      "Epoch 92/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0179 - binary_crossentropy: 0.0179 - val_loss: 0.0194 - val_binary_crossentropy: 0.0194\n",
      "Epoch 93/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0268 - binary_crossentropy: 0.0268 - val_loss: 0.0196 - val_binary_crossentropy: 0.0196\n",
      "Epoch 94/200\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0284 - binary_crossentropy: 0.0284 - val_loss: 0.0159 - val_binary_crossentropy: 0.0159\n",
      "Epoch 95/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0277 - binary_crossentropy: 0.0277 - val_loss: 0.0159 - val_binary_crossentropy: 0.0159\n",
      "Epoch 96/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0263 - binary_crossentropy: 0.0263 - val_loss: 0.0122 - val_binary_crossentropy: 0.0122\n",
      "Epoch 97/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0169 - binary_crossentropy: 0.0169 - val_loss: 0.0107 - val_binary_crossentropy: 0.0107\n",
      "Epoch 98/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0153 - binary_crossentropy: 0.0153 - val_loss: 0.0097 - val_binary_crossentropy: 0.0097\n",
      "Epoch 99/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0152 - binary_crossentropy: 0.0152 - val_loss: 0.0101 - val_binary_crossentropy: 0.0101\n",
      "Epoch 100/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0184 - binary_crossentropy: 0.0184 - val_loss: 0.0100 - val_binary_crossentropy: 0.0100\n",
      "Epoch 101/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0174 - binary_crossentropy: 0.0174 - val_loss: 0.0094 - val_binary_crossentropy: 0.0094\n",
      "Epoch 102/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0202 - binary_crossentropy: 0.0202 - val_loss: 0.0099 - val_binary_crossentropy: 0.0099\n",
      "Epoch 103/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0159 - binary_crossentropy: 0.0159 - val_loss: 0.0094 - val_binary_crossentropy: 0.0094\n",
      "Epoch 104/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0121 - binary_crossentropy: 0.0121 - val_loss: 0.0086 - val_binary_crossentropy: 0.0086\n",
      "Epoch 105/200\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0165 - binary_crossentropy: 0.0165 - val_loss: 0.0097 - val_binary_crossentropy: 0.0097\n",
      "Epoch 106/200\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0104 - binary_crossentropy: 0.0104 - val_loss: 0.0092 - val_binary_crossentropy: 0.0092\n",
      "Epoch 107/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0161 - binary_crossentropy: 0.0161 - val_loss: 0.0082 - val_binary_crossentropy: 0.0082\n",
      "Epoch 108/200\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0139 - binary_crossentropy: 0.0139 - val_loss: 0.0083 - val_binary_crossentropy: 0.0083\n",
      "Epoch 109/200\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0192 - binary_crossentropy: 0.0192 - val_loss: 0.0077 - val_binary_crossentropy: 0.0077\n",
      "Epoch 110/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0161 - binary_crossentropy: 0.0161 - val_loss: 0.0080 - val_binary_crossentropy: 0.0080\n",
      "Epoch 111/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0139 - binary_crossentropy: 0.0139 - val_loss: 0.0079 - val_binary_crossentropy: 0.0079\n",
      "Epoch 112/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0222 - binary_crossentropy: 0.0222 - val_loss: 0.0131 - val_binary_crossentropy: 0.0131\n",
      "Epoch 113/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0164 - binary_crossentropy: 0.0164 - val_loss: 0.0128 - val_binary_crossentropy: 0.0128\n",
      "Epoch 114/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0144 - binary_crossentropy: 0.0144 - val_loss: 0.0120 - val_binary_crossentropy: 0.0120\n",
      "Epoch 115/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0193 - binary_crossentropy: 0.0193 - val_loss: 0.0119 - val_binary_crossentropy: 0.0119\n",
      "Epoch 116/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0104 - binary_crossentropy: 0.0104 - val_loss: 0.0119 - val_binary_crossentropy: 0.0119\n",
      "Epoch 117/200\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0257 - binary_crossentropy: 0.0257 - val_loss: 0.0094 - val_binary_crossentropy: 0.0094\n",
      "Epoch 118/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0140 - binary_crossentropy: 0.0140 - val_loss: 0.0074 - val_binary_crossentropy: 0.0074\n",
      "Epoch 119/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0241 - binary_crossentropy: 0.0241 - val_loss: 0.0073 - val_binary_crossentropy: 0.0073\n",
      "Epoch 120/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0188 - binary_crossentropy: 0.0188 - val_loss: 0.0077 - val_binary_crossentropy: 0.0077\n",
      "Epoch 121/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0258 - binary_crossentropy: 0.0258 - val_loss: 0.0061 - val_binary_crossentropy: 0.0061\n",
      "Epoch 122/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0108 - binary_crossentropy: 0.0108 - val_loss: 0.0065 - val_binary_crossentropy: 0.0065\n",
      "Epoch 123/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0122 - binary_crossentropy: 0.0122 - val_loss: 0.0060 - val_binary_crossentropy: 0.0060\n",
      "Epoch 124/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0143 - binary_crossentropy: 0.0143 - val_loss: 0.0062 - val_binary_crossentropy: 0.0062\n",
      "Epoch 125/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0155 - binary_crossentropy: 0.0155 - val_loss: 0.0068 - val_binary_crossentropy: 0.0068\n",
      "Epoch 126/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0117 - binary_crossentropy: 0.0117 - val_loss: 0.0064 - val_binary_crossentropy: 0.0064\n",
      "Epoch 127/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0140 - binary_crossentropy: 0.0140 - val_loss: 0.0075 - val_binary_crossentropy: 0.0075\n",
      "Epoch 128/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0204 - binary_crossentropy: 0.0204 - val_loss: 0.0063 - val_binary_crossentropy: 0.0063\n",
      "Epoch 129/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0126 - binary_crossentropy: 0.0126 - val_loss: 0.0042 - val_binary_crossentropy: 0.0042\n",
      "Epoch 130/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0134 - binary_crossentropy: 0.0134 - val_loss: 0.0041 - val_binary_crossentropy: 0.0041\n",
      "Epoch 131/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0328 - binary_crossentropy: 0.0328 - val_loss: 0.0058 - val_binary_crossentropy: 0.0058\n",
      "Epoch 132/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0115 - binary_crossentropy: 0.0115 - val_loss: 0.0058 - val_binary_crossentropy: 0.0058\n",
      "Epoch 133/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0202 - binary_crossentropy: 0.0202 - val_loss: 0.0059 - val_binary_crossentropy: 0.0059\n",
      "Epoch 134/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0188 - binary_crossentropy: 0.0188 - val_loss: 0.0048 - val_binary_crossentropy: 0.0048\n",
      "Epoch 135/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0151 - binary_crossentropy: 0.0151 - val_loss: 0.0060 - val_binary_crossentropy: 0.0060\n",
      "Epoch 136/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0128 - binary_crossentropy: 0.0128 - val_loss: 0.0060 - val_binary_crossentropy: 0.0060\n",
      "Epoch 137/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0171 - binary_crossentropy: 0.0171 - val_loss: 0.0065 - val_binary_crossentropy: 0.0065\n",
      "Epoch 138/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0111 - binary_crossentropy: 0.0111 - val_loss: 0.0068 - val_binary_crossentropy: 0.0068\n",
      "Epoch 139/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0127 - binary_crossentropy: 0.0127 - val_loss: 0.0064 - val_binary_crossentropy: 0.0064\n",
      "Epoch 140/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0171 - binary_crossentropy: 0.0171 - val_loss: 0.0059 - val_binary_crossentropy: 0.0059\n",
      "Epoch 141/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0080 - binary_crossentropy: 0.0080 - val_loss: 0.0057 - val_binary_crossentropy: 0.0057\n",
      "Epoch 142/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0217 - binary_crossentropy: 0.0217 - val_loss: 0.0039 - val_binary_crossentropy: 0.0039\n",
      "Epoch 143/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0116 - binary_crossentropy: 0.0116 - val_loss: 0.0035 - val_binary_crossentropy: 0.0035\n",
      "Epoch 144/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0144 - binary_crossentropy: 0.0144 - val_loss: 0.0035 - val_binary_crossentropy: 0.0035\n",
      "Epoch 145/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0086 - binary_crossentropy: 0.0086 - val_loss: 0.0033 - val_binary_crossentropy: 0.0033\n",
      "Epoch 146/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0108 - binary_crossentropy: 0.0108 - val_loss: 0.0038 - val_binary_crossentropy: 0.0038\n",
      "Epoch 147/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0161 - binary_crossentropy: 0.0161 - val_loss: 0.0039 - val_binary_crossentropy: 0.0039\n",
      "Epoch 148/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0160 - binary_crossentropy: 0.0160 - val_loss: 0.0048 - val_binary_crossentropy: 0.0048\n",
      "Epoch 149/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0121 - binary_crossentropy: 0.0121 - val_loss: 0.0051 - val_binary_crossentropy: 0.0051\n",
      "Epoch 150/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0097 - binary_crossentropy: 0.0097 - val_loss: 0.0039 - val_binary_crossentropy: 0.0039\n",
      "Epoch 151/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0084 - binary_crossentropy: 0.0084 - val_loss: 0.0043 - val_binary_crossentropy: 0.0043\n",
      "Epoch 152/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0084 - binary_crossentropy: 0.0084 - val_loss: 0.0044 - val_binary_crossentropy: 0.0044\n",
      "Epoch 153/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0096 - binary_crossentropy: 0.0096 - val_loss: 0.0048 - val_binary_crossentropy: 0.0048\n",
      "Epoch 154/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0123 - binary_crossentropy: 0.0123 - val_loss: 0.0044 - val_binary_crossentropy: 0.0044\n",
      "Epoch 155/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0137 - binary_crossentropy: 0.0137 - val_loss: 0.0042 - val_binary_crossentropy: 0.0042\n",
      "Epoch 156/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0161 - binary_crossentropy: 0.0161 - val_loss: 0.0048 - val_binary_crossentropy: 0.0048\n",
      "Epoch 157/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0134 - binary_crossentropy: 0.0134 - val_loss: 0.0046 - val_binary_crossentropy: 0.0046\n",
      "Epoch 158/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0106 - binary_crossentropy: 0.0106 - val_loss: 0.0041 - val_binary_crossentropy: 0.0041\n",
      "Epoch 159/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0106 - binary_crossentropy: 0.0106 - val_loss: 0.0041 - val_binary_crossentropy: 0.0041\n",
      "Epoch 160/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0086 - binary_crossentropy: 0.0086 - val_loss: 0.0033 - val_binary_crossentropy: 0.0033\n",
      "Epoch 161/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0129 - binary_crossentropy: 0.0129 - val_loss: 0.0026 - val_binary_crossentropy: 0.0026\n",
      "Epoch 162/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0108 - binary_crossentropy: 0.0108 - val_loss: 0.0023 - val_binary_crossentropy: 0.0023\n",
      "Epoch 163/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0114 - binary_crossentropy: 0.0114 - val_loss: 0.0022 - val_binary_crossentropy: 0.0022\n",
      "Epoch 164/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0127 - binary_crossentropy: 0.0127 - val_loss: 0.0034 - val_binary_crossentropy: 0.0034\n",
      "Epoch 165/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0076 - binary_crossentropy: 0.0076 - val_loss: 0.0034 - val_binary_crossentropy: 0.0034\n",
      "Epoch 166/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0097 - binary_crossentropy: 0.0097 - val_loss: 0.0025 - val_binary_crossentropy: 0.0025\n",
      "Epoch 167/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0100 - binary_crossentropy: 0.0100 - val_loss: 0.0019 - val_binary_crossentropy: 0.0019\n",
      "Epoch 168/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0090 - binary_crossentropy: 0.0090 - val_loss: 0.0019 - val_binary_crossentropy: 0.0019\n",
      "Epoch 169/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0069 - binary_crossentropy: 0.0069 - val_loss: 0.0019 - val_binary_crossentropy: 0.0019\n",
      "Epoch 170/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0141 - binary_crossentropy: 0.0141 - val_loss: 0.0027 - val_binary_crossentropy: 0.0027\n",
      "Epoch 171/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0125 - binary_crossentropy: 0.0125 - val_loss: 0.0024 - val_binary_crossentropy: 0.0024\n",
      "Epoch 172/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0069 - binary_crossentropy: 0.0069 - val_loss: 0.0023 - val_binary_crossentropy: 0.0023\n",
      "Epoch 173/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0120 - binary_crossentropy: 0.0120 - val_loss: 0.0025 - val_binary_crossentropy: 0.0025\n",
      "Epoch 174/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0071 - binary_crossentropy: 0.0071 - val_loss: 0.0022 - val_binary_crossentropy: 0.0022\n",
      "Epoch 175/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0061 - binary_crossentropy: 0.0061 - val_loss: 0.0023 - val_binary_crossentropy: 0.0023\n",
      "Epoch 176/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0056 - binary_crossentropy: 0.0056 - val_loss: 0.0024 - val_binary_crossentropy: 0.0024\n",
      "Epoch 177/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0092 - binary_crossentropy: 0.0092 - val_loss: 0.0021 - val_binary_crossentropy: 0.0021\n",
      "Epoch 178/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0078 - binary_crossentropy: 0.0078 - val_loss: 0.0020 - val_binary_crossentropy: 0.0020\n",
      "Epoch 179/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0100 - binary_crossentropy: 0.0100 - val_loss: 0.0014 - val_binary_crossentropy: 0.0014\n",
      "Epoch 180/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0088 - binary_crossentropy: 0.0088 - val_loss: 0.0015 - val_binary_crossentropy: 0.0015\n",
      "Epoch 181/200\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0118 - binary_crossentropy: 0.0118 - val_loss: 0.0015 - val_binary_crossentropy: 0.0015\n",
      "Epoch 182/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0089 - binary_crossentropy: 0.0089 - val_loss: 0.0014 - val_binary_crossentropy: 0.0014\n",
      "Epoch 183/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0108 - binary_crossentropy: 0.0108 - val_loss: 0.0018 - val_binary_crossentropy: 0.0018\n",
      "Epoch 184/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0056 - binary_crossentropy: 0.0056 - val_loss: 0.0019 - val_binary_crossentropy: 0.0019\n",
      "Epoch 185/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0070 - binary_crossentropy: 0.0070 - val_loss: 0.0018 - val_binary_crossentropy: 0.0018\n",
      "Epoch 186/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0221 - binary_crossentropy: 0.0221 - val_loss: 0.0034 - val_binary_crossentropy: 0.0034\n",
      "Epoch 187/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0090 - binary_crossentropy: 0.0090 - val_loss: 0.0033 - val_binary_crossentropy: 0.0033\n",
      "Epoch 188/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0079 - binary_crossentropy: 0.0079 - val_loss: 0.0033 - val_binary_crossentropy: 0.0033\n",
      "Epoch 189/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0073 - binary_crossentropy: 0.0073 - val_loss: 0.0032 - val_binary_crossentropy: 0.0032\n",
      "Epoch 190/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0077 - binary_crossentropy: 0.0077 - val_loss: 0.0031 - val_binary_crossentropy: 0.0031\n",
      "Epoch 191/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0107 - binary_crossentropy: 0.0107 - val_loss: 0.0038 - val_binary_crossentropy: 0.0038\n",
      "Epoch 192/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0083 - binary_crossentropy: 0.0083 - val_loss: 0.0040 - val_binary_crossentropy: 0.0040\n",
      "Epoch 193/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0211 - binary_crossentropy: 0.0211 - val_loss: 0.0039 - val_binary_crossentropy: 0.0039\n",
      "Epoch 194/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0142 - binary_crossentropy: 0.0142 - val_loss: 0.0078 - val_binary_crossentropy: 0.0078\n",
      "Epoch 195/200\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0043 - binary_crossentropy: 0.0043 - val_loss: 0.0079 - val_binary_crossentropy: 0.0079\n",
      "Epoch 196/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0088 - binary_crossentropy: 0.0088 - val_loss: 0.0071 - val_binary_crossentropy: 0.0071\n",
      "Epoch 197/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0090 - binary_crossentropy: 0.0090 - val_loss: 0.0061 - val_binary_crossentropy: 0.0061\n",
      "Epoch 198/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0043 - binary_crossentropy: 0.0043 - val_loss: 0.0064 - val_binary_crossentropy: 0.0064\n",
      "Epoch 199/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0109 - binary_crossentropy: 0.0109 - val_loss: 0.0054 - val_binary_crossentropy: 0.0054\n",
      "Epoch 200/200\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0060 - binary_crossentropy: 0.0060 - val_loss: 0.0051 - val_binary_crossentropy: 0.0051\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x13e4fff40>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import BatchNormalization, Dense, Dropout\n",
    "from keras import optimizers\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(units=train_x.shape[1], activation='relu'), # 入力層は列数分のユニット\n",
    "    Dropout(rate=0.2),\n",
    "    BatchNormalization(),\n",
    "    Dense(units=16, activation='relu'),\n",
    "    Dropout(rate=0.2),\n",
    "    Dense(units=1, activation='sigmoid')              # 出力層は1ユニット\n",
    "])\n",
    "\n",
    "model.compile(optimizer=optimizers.SGD(learning_rate=0.01), loss='binary_crossentropy', metrics=['binary_crossentropy'])\n",
    "model.fit(x=train_x, y=train_y, validation_data=(valid_x, valid_y), batch_size=32, epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "52b5fcf3-227c-4c1e-ae9f-a51aa94341ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Age_isnull</th>\n",
       "      <th>Fare_isnull</th>\n",
       "      <th>Sex_male</th>\n",
       "      <th>Sex_nan</th>\n",
       "      <th>Cabin_A18</th>\n",
       "      <th>...</th>\n",
       "      <th>Cabin_F E57</th>\n",
       "      <th>Cabin_F G63</th>\n",
       "      <th>Cabin_F2</th>\n",
       "      <th>Cabin_F33</th>\n",
       "      <th>Cabin_F4</th>\n",
       "      <th>Cabin_G6</th>\n",
       "      <th>Cabin_nan</th>\n",
       "      <th>Embarked_Q</th>\n",
       "      <th>Embarked_S</th>\n",
       "      <th>Embarked_nan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.49947</td>\n",
       "      <td>3.680326</td>\n",
       "      <td>-0.182174</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.051760</td>\n",
       "      <td>-0.49947</td>\n",
       "      <td>-0.400248</td>\n",
       "      <td>-0.497213</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>0.5</td>\n",
       "      <td>-0.576302</td>\n",
       "      <td>-0.49947</td>\n",
       "      <td>-0.400248</td>\n",
       "      <td>-0.450521</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.49947</td>\n",
       "      <td>-0.400248</td>\n",
       "      <td>-0.509240</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.655545</td>\n",
       "      <td>-0.49947</td>\n",
       "      <td>-0.400248</td>\n",
       "      <td>-0.500275</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.734788</td>\n",
       "      <td>-0.49947</td>\n",
       "      <td>-0.400248</td>\n",
       "      <td>-0.497213</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.734788</td>\n",
       "      <td>-0.49947</td>\n",
       "      <td>-0.400248</td>\n",
       "      <td>-0.498558</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.246287</td>\n",
       "      <td>-0.49947</td>\n",
       "      <td>-0.400248</td>\n",
       "      <td>0.710273</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>0.5</td>\n",
       "      <td>-0.497059</td>\n",
       "      <td>-0.49947</td>\n",
       "      <td>-0.400248</td>\n",
       "      <td>-0.396732</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.49947</td>\n",
       "      <td>-0.400248</td>\n",
       "      <td>-0.499827</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>313 rows × 88 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Pclass       Age    SibSp     Parch      Fare  Age_isnull  Fare_isnull  \\\n",
       "132     1.0  0.000000 -0.49947  3.680326 -0.182174           1            0   \n",
       "60      1.0 -1.051760 -0.49947 -0.400248 -0.497213           0            0   \n",
       "198     0.5 -0.576302 -0.49947 -0.400248 -0.450521           0            0   \n",
       "332     1.0  0.000000 -0.49947 -0.400248 -0.509240           1            0   \n",
       "63      1.0 -0.655545 -0.49947 -0.400248 -0.500275           0            0   \n",
       "..      ...       ...      ...       ...       ...         ...          ...   \n",
       "71      1.0 -0.734788 -0.49947 -0.400248 -0.497213           0            0   \n",
       "106     1.0 -0.734788 -0.49947 -0.400248 -0.498558           0            0   \n",
       "270     0.0  1.246287 -0.49947 -0.400248  0.710273           0            0   \n",
       "348     0.5 -0.497059 -0.49947 -0.400248 -0.396732           0            0   \n",
       "102     1.0  0.000000 -0.49947 -0.400248 -0.499827           1            0   \n",
       "\n",
       "     Sex_male  Sex_nan  Cabin_A18  ...  Cabin_F E57  Cabin_F G63  Cabin_F2  \\\n",
       "132         0        0          0  ...            0            0         0   \n",
       "60          1        0          0  ...            0            0         0   \n",
       "198         1        0          0  ...            0            0         0   \n",
       "332         1        0          0  ...            0            0         0   \n",
       "63          0        0          0  ...            0            0         0   \n",
       "..        ...      ...        ...  ...          ...          ...       ...   \n",
       "71          1        0          0  ...            0            0         0   \n",
       "106         1        0          0  ...            0            0         0   \n",
       "270         1        0          0  ...            0            0         0   \n",
       "348         1        0          0  ...            0            0         0   \n",
       "102         1        0          0  ...            0            0         0   \n",
       "\n",
       "     Cabin_F33  Cabin_F4  Cabin_G6  Cabin_nan  Embarked_Q  Embarked_S  \\\n",
       "132          0         0         0          1           0           1   \n",
       "60           0         0         0          1           0           1   \n",
       "198          0         0         0          1           0           1   \n",
       "332          0         0         0          1           0           0   \n",
       "63           0         0         0          1           1           0   \n",
       "..         ...       ...       ...        ...         ...         ...   \n",
       "71           0         0         0          1           0           1   \n",
       "106          0         0         0          1           1           0   \n",
       "270          0         0         0          0           0           0   \n",
       "348          0         0         0          1           0           1   \n",
       "102          0         0         0          1           1           0   \n",
       "\n",
       "     Embarked_nan  \n",
       "132             0  \n",
       "60              0  \n",
       "198             0  \n",
       "332             0  \n",
       "63              0  \n",
       "..            ...  \n",
       "71              0  \n",
       "106             0  \n",
       "270             0  \n",
       "348             0  \n",
       "102             0  \n",
       "\n",
       "[313 rows x 88 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5af2190-9869-4e3a-850d-859faf51143e",
   "metadata": {},
   "source": [
    "## バリデーションデータで評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6a750d-fe42-4739-a3a0-c092fd30a6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499d53e1-7e62-462e-9614-b49a7df4e863",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = model.predict_proba(train_x)\n",
    "y_pred = model.predict_proba(valid_x)\n",
    "\n",
    "logloss_train = log_loss(y_true=train_y, y_pred=y_pred_train)\n",
    "logloss_valid = log_loss(y_true=valid_y, y_pred=y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9534ff-c477-4eec-a427-3c7132e0a4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"logloss - train: {logloss_train}\")\n",
    "print(f\"logloss - valid: {logloss_valid}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744c65ad-7343-4452-9b60-a16500b7c62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "joblib.dump(model, 'model.joblib')\n",
    "\n",
    "model = joblib.load('model.joblib')\n",
    "m2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b77b4b-e9ba-4160-aabc-f16de2d59622",
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_test_df = preprocess(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ce681f-25e5-44d3-832f-cd1b4a1c5c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict_proba(prep_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1f3853-f1af-4b73-bcaf-39dba5d9af51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba8bd62-bfb3-4e46-afd7-5239fee17e4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
